{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31544cc8-b726-48fe-953a-b538c4021548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b8033-edd9-4f1d-80c6-8e1822434b7a",
   "metadata": {},
   "source": [
    " Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ecda6-6043-4579-a54e-8f0ba54f9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open(\"/content/datascience.txt\", 'r', encoding='utf-8') as file:\n",
    "    datascience = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb77a09-2e2f-4285-9a55-ad57cec3ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "datascience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af7bde-5d7c-491d-91c0-c48244d5a9e0",
   "metadata": {},
   "source": [
    "How to Learn AI From Scratch in 2024: A Complete Guide From the Experts\r\n",
    "\r\n",
    "Find out everything you need to know about learning AI in 2024, from tips to get you started, helpful resources, and insights from industry experts.\r\n",
    "\r\n",
    "We’re living through what is quite possibly a pivotal point in human history, where the importance of Artificial Intelligence (AI) is becoming increasingly undeniable. Just consider this statistic: 97% of business owners expect that ChatGPT will bring about positive changes in at least one area of their business, according to a survey by Forbes Advisor. Moreover, tools like ChatGPT, Midjourney, and Bard are ushering AI into the mainstream. This makes the art and science of AI more relevant than ever before.\r\n",
    "\r\n",
    "If you're an aspiring data scientist, machine learning engineer, AI researcher, or simply an AI enthusiast, this guide is for you. Throughout this article, we'll detail how to learn AI from scratch and offer insights from industry experts to help steer your journey. As well as covering the skills and tools you’ll need to master, we also explore how businesses can leverage AI in today’s landscape.\r\n",
    "\r\n",
    "What is Artificial Intelligence (AI)?\r\n",
    "\r\n",
    "AI, or Artificial Intelligence, is a branch of computer science focused on creating systems that can perform tasks that would normally require human intelligence. These tasks range from understanding natural language, recognizing patterns, making decisions, and learning from experience. AI is a broad field with numerous subfields, each with its unique objectives and specializations. Check out our full guide, What is AI? to find out more. You can also explore how AI is different from machine learning in a separate article. \r\n",
    "\r\n",
    "What are the different types of artificial intelligence?\r\n",
    "\r\n",
    "As AI grows in popularity, the technology is discussed in various ways. To simplify the remainder of the article, it’s important to look at the different types of AI. AI can be categorized into three levels based on its capabilities:\r\n",
    "\r\n",
    "Artificial Narrow Intelligence (ANI): This is the most common form of AI we interact with today. ANI is designed to perform a single task, like voice recognition or recommendations on streaming services.\r\n",
    "\r\n",
    "Artificial General Intelligence (AGI): \r\n",
    "\r\n",
    "An AI with AGI possesses the ability to understand, learn, adapt, and implement knowledge across a wide range of tasks at a human level. While large language models and tools such as ChatGPT have shown the ability to generalize across many tasks—as of 2023, this is still a theoretical concept.\r\n",
    "\r\n",
    "Artificial Super Intelligence (ASI): The final level of AI, ASI, refers to a future scenario where AI surpasses human intelligence in nearly all economically valuable work. This concept, while intriguing, remains largely speculative.\r\n",
    "\r\n",
    "The difference between data science, artificial intelligence, machine learning & deep learning\r\n",
    "If you are new to this topic, you may also see the terms “machine learning,” “deep learning,” “data science,” and others creep into AI discourse. AI is a broad field with several subsets, including Machine Learning (ML) and Deep Learning (DL).\r\n",
    "\r\n",
    "While there isn't an official definition for any of these terms, and while experts argue over the exact boundaries, there is a growing consensus on the broad scope of each term. Here’s a breakdown of how these terms can be defined:\r\n",
    "\r\n",
    "Artificial intelligence refers to computer systems that can behave intelligently, reason, and learn like humans.\r\n",
    "\r\n",
    "Machine learning is a subset of artificial intelligence focused on developing algorithms with the ability to learn without explicitly being programmed.\r\n",
    "\r\n",
    "Deep learning is a subset of machine learning. It is responsible for many of the awe-inspiring news stories about AI in the news (e.g., self-driving cars, ChatGPT). Deep learning algorithms are inspired by the brain's structure and work exceptionally well with unstructured data such as images, videos, or text.\r\n",
    "\r\n",
    "Data science is a cross-disciplinary field that uses all of the above, amongst other skills like data analysis, statistics, data visualization, and more, to get insight from data.\r\n",
    "\r\n",
    "hy Should You Learn Artificial Intelligence Right Now?\r\n",
    "\r\n",
    "Artificial Intelligence is more than just a buzzword; it's a revolutionary technology transforming how we work, live, and interact. With the explosion of data and the need to make sense of it, the demand for AI skills is skyrocketing. There's no better time than now to start learning AI. Here's why:\r\n",
    "\r\n",
    "AI is a fast-growing field\r\n",
    "\r\n",
    "Artificial Intelligence isn't the future; it's the present. The number of AI jobs has seen significant growth over recent years. According to the World Economic Forum’s Future of Jobs report, ​​AI and machine learning specialists top the list of fast-growing jobs over the next five years. As industries continue to adopt AI technologies to streamline their operations and make better decisions, the demand for AI specialists will likely only increase.\r\n",
    "\r\n",
    "AI is a high-paying job\r\n",
    "\r\n",
    "Naturally, the surge in demand for AI skills comes with attractive compensation. According to data from Glassdoor, as of November 2023, the average salary of an AI engineer in the United States is $153,719 per annum, with the potential for bonuses and profit sharing. Machine learning engineers and data scientists are similarly well-paid, with average salaries of $151,158 and $178,515 per annum, respectively. This financial compensation reflects the value and impact of AI skills in the marketplace.\r\n",
    "\r\n",
    "What we've also seen is an increase in these averages from May 2023 when we wrote the article, to its last update in November 2023. The average salary on Glassdoor was reported as $128,479 in May, and $153,719 in November. \r\n",
    "\r\n",
    "AI is a high-paying job\r\n",
    "\r\n",
    "Naturally, the surge in demand for AI skills comes with attractive compensation. According to data from Glassdoor, as of November 2023, the average salary of an AI engineer in the United States is $153,719 per annum, with the potential for bonuses and profit sharing. Machine learning engineers and data scientists are similarly well-paid, with average salaries of $151,158 and $178,515 per annum, respectively. This financial compensation reflects the value and impact of AI skills in the marketplace.\r\n",
    "\r\n",
    "What we've also seen is an increase in these averages from May 2023 when we wrote the article, to its last update in November 2023. The average salary on Glassdoor was reported as $128,479 in May, and $153,719 in November. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea1b5b-50cc-4bff-ba3c-2e094d033f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts([datascience])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc67459-854e-4e41-8b62-2a1573b2871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_config()['filters']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53cd8d0-6483-4e7d-9b0d-0f79272eb375",
   "metadata": {},
   "source": [
    "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c017dd-80c5-4036-aee6-6bdfc0690f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_config()['word_counts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24da97d-2468-4eec-89b1-62574371821a",
   "metadata": {},
   "source": [
    "{\"how\": 6, \"to\": 27, \"learn\": 6, \"ai\": 38, \"from\": 14, \"scratch\": 2, \"in\": 24, \"2024\": 2, \"a\": 22, \"complete\": 1, \"guide\": 3, \"the\": 49, \"experts\": 4, \"find\": 2, \"out\": 3, \"everything\": 1, \"you\": 7, \"need\": 3, \"know\": 1, \"about\": 3, \"learning\": 18, \"tips\": 1, \"get\": 2, \"started\": 1, \"helpful\": 1, \"resources\": 1, \"and\": 29, \"insights\": 2, \"industry\": 2, \"we\\u2019re\": 1, \"living\": 1, \"through\": 1, \"what\": 6, \"is\": 27, \"quite\": 1, \"possibly\": 1, \"pivotal\": 1, \"point\": 1, \"human\": 4, \"history\": 1, \"where\": 2, \"importance\": 1, \"of\": 32, \"artificial\": 13, \"intelligence\": 15, \"becoming\": 1, \"increasingly\": 1, \"undeniable\": 1, \"just\": 2, \"consider\": 1, \"this\": 10, \"statistic\": 1, \"97\": 1, \"business\": 2, \"owners\": 1, \"expect\": 1, \"that\": 5, \"chatgpt\": 4, \"will\": 2, \"bring\": 1, \"positive\": 1, \"changes\": 1, \"at\": 3, \"least\": 1, \"one\": 1, \"area\": 1, \"their\": 2, \"according\": 4, \"survey\": 1, \"by\": 2, \"forbes\": 1, \"advisor\": 1, \"moreover\": 1, \"tools\": 3, \"like\": 4, \"midjourney\": 1, \"bard\": 1, \"are\": 6, \"ushering\": 1, \"into\": 3, \"mainstream\": 1, \"makes\": 1, \"art\": 1, \"science\": 5, \"more\": 4, \"relevant\": 1, \"than\": 3, \"ever\": 1, \"before\": 1, \"if\": 2, \"you're\": 1, \"an\": 8, \"aspiring\": 1, \"data\": 12, \"scientist\": 1, \"machine\": 9, \"engineer\": 3, \"researcher\": 1, \"or\": 4, \"simply\": 1, \"enthusiast\": 1, \"for\": 9, \"throughout\": 1, \"article\": 5, \"we'll\": 1, \"detail\": 1, \"offer\": 1, \"help\": 1, \"steer\": 1, \"your\": 1, \"journey\": 1, \"as\": 10, \"well\": 4, \"covering\": 1, \"skills\": 7, \"you\\u2019ll\": 1, \"master\": 1, \"we\": 5, \"also\": 5, \"explore\": 2, \"businesses\": 1, \"can\": 6, \"leverage\": 1, \"today\\u2019s\": 1, \"landscape\": 1, \"branch\": 1, \"computer\": 2, \"focused\": 2, \"on\": 7, \"creating\": 1, \"systems\": 2, \"perform\": 2, \"tasks\": 3, \"would\": 1, \"normally\": 1, \"require\": 1, \"these\": 5, \"range\": 2, \"understanding\": 1, \"natural\": 1, \"language\": 2, \"recognizing\": 1, \"patterns\": 1, \"making\": 1, \"decisions\": 2, \"experience\": 1, \"broad\": 3, \"field\": 4, \"with\": 14, \"numerous\": 1, \"subfields\": 1, \"each\": 2, \"its\": 4, \"unique\": 1, \"objectives\": 1, \"specializations\": 1, \"check\": 1, \"our\": 1, \"full\": 1, \"different\": 3, \"separate\": 1, \"types\": 2, \"grows\": 1, \"popularity\": 1, \"technology\": 2, \"discussed\": 1, \"various\": 1, \"ways\": 1, \"simplify\": 1, \"remainder\": 1, \"it\\u2019s\": 1, \"important\": 1, \"look\": 1, \"be\": 2, \"categorized\": 1, \"three\": 1, \"levels\": 1, \"based\": 1, \"capabilities\": 1, \"narrow\": 1, \"ani\": 2, \"most\": 1, \"common\": 1, \"form\": 1, \"interact\": 2, \"today\": 1, \"designed\": 1, \"single\": 1, \"task\": 1, \"voice\": 1, \"recognition\": 1, \"recommendations\": 1, \"streaming\": 1, \"services\": 1, \"general\": 1, \"agi\": 2, \"possesses\": 1, \"ability\": 3, \"understand\": 1, \"adapt\": 1, \"implement\": 1, \"knowledge\": 1, \"across\": 2, \"wide\": 1, \"level\": 2, \"while\": 4, \"large\": 1, \"models\": 1, \"such\": 2, \"have\": 1, \"shown\": 1, \"generalize\": 1, \"many\": 2, \"tasks\\u2014as\": 1, \"2023\": 7, \"still\": 1, \"theoretical\": 1, \"concept\": 2, \"super\": 1, \"asi\": 2, \"final\": 1, \"refers\": 2, \"future\": 3, \"scenario\": 1, \"surpasses\": 1, \"nearly\": 1, \"all\": 2, \"economically\": 1, \"valuable\": 1, \"work\": 3, \"intriguing\": 1, \"remains\": 1, \"largely\": 1, \"speculative\": 1, \"difference\": 1, \"between\": 1, \"deep\": 4, \"new\": 1, \"topic\": 1, \"may\": 5, \"see\": 1, \"terms\": 3, \"\\u201cmachine\": 1, \"\\u201d\": 3, \"\\u201cdeep\": 1, \"\\u201cdata\": 1, \"others\": 1, \"creep\": 1, \"discourse\": 1, \"several\": 1, \"subsets\": 1, \"including\": 1, \"ml\": 1, \"dl\": 1, \"there\": 2, \"isn't\": 2, \"official\": 1, \"definition\": 1, \"any\": 1, \"argue\": 1, \"over\": 3, \"exact\": 1, \"boundaries\": 1, \"growing\": 3, \"consensus\": 1, \"scope\": 1, \"term\": 1, \"here\\u2019s\": 1, \"breakdown\": 1, \"defined\": 1, \"behave\": 1, \"intelligently\": 1, \"reason\": 1, \"humans\": 1, \"subset\": 2, \"developing\": 1, \"algorithms\": 2, \"without\": 1, \"explicitly\": 1, \"being\": 1, \"programmed\": 1, \"it\": 2, \"responsible\": 1, \"awe\": 1, \"inspiring\": 1, \"news\": 2, \"stories\": 1, \"e\": 1, \"g\": 1, \"self\": 1, \"driving\": 1, \"cars\": 1, \"inspired\": 1, \"brain's\": 1, \"structure\": 1, \"exceptionally\": 1, \"unstructured\": 1, \"images\": 1, \"videos\": 1, \"text\": 1, \"cross\": 1, \"disciplinary\": 1, \"uses\": 1, \"above\": 1, \"amongst\": 1, \"other\": 1, \"analysis\": 1, \"statistics\": 1, \"visualization\": 1, \"insight\": 1, \"hy\": 1, \"should\": 1, \"right\": 1, \"now\": 2, \"buzzword\": 1, \"it's\": 2, \"revolutionary\": 1, \"transforming\": 1, \"live\": 1, \"explosion\": 1, \"make\": 2, \"sense\": 1, \"demand\": 4, \"skyrocketing\": 1, \"there's\": 1, \"no\": 1, \"better\": 2, \"time\": 1, \"start\": 1, \"here's\": 1, \"why\": 1, \"fast\": 2, \"present\": 1, \"number\": 1, \"jobs\": 3, \"has\": 1, \"seen\": 3, \"significant\": 1, \"growth\": 1, \"recent\": 1, \"years\": 2, \"world\": 1, \"economic\": 1, \"forum\\u2019s\": 1, \"report\": 1, \"\\u200b\\u200bai\": 1, \"specialists\": 2, \"top\": 1, \"list\": 1, \"next\": 1, \"five\": 1, \"industries\": 1, \"continue\": 1, \"adopt\": 1, \"technologies\": 1, \"streamline\": 1, \"operations\": 1, \"likely\": 1, \"only\": 1, \"increase\": 3, \"high\": 2, \"paying\": 2, \"job\": 2, \"naturally\": 2, \"surge\": 2, \"comes\": 2, \"attractive\": 2, \"compensation\": 4, \"glassdoor\": 4, \"november\": 6, \"average\": 6, \"salary\": 4, \"united\": 2, \"states\": 2, \"153\": 4, \"719\": 4, \"per\": 4, \"annum\": 4, \"potential\": 2, \"bonuses\": 2, \"profit\": 2, \"sharing\": 2, \"engineers\": 2, \"scientists\": 2, \"similarly\": 2, \"paid\": 2, \"salaries\": 2, \"151\": 2, \"158\": 2, \"178\": 2, \"515\": 2, \"respectively\": 2, \"financial\": 2, \"reflects\": 2, \"value\": 2, \"impact\": 2, \"marketplace\": 2, \"we've\": 2, \"averages\": 2, \"when\": 2, \"wrote\": 2, \"last\": 2, \"update\": 2, \"was\": 2, \"reported\": 2, \"128\": 2, \"479\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af710711-5c60-4730-9183-c203092a5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459ffe3-cac2-4b9b-b004-e8c66deb5a9e",
   "metadata": {},
   "source": [
    "{'the': 1,\r\n",
    " 'ai': 2,\r\n",
    " 'of': 3,\r\n",
    " 'and': 4,\r\n",
    " 'to': 5,\r\n",
    " 'is': 6,\r\n",
    " 'in': 7,\r\n",
    " 'a': 8,\r\n",
    " 'learning': 9,\r\n",
    " 'intelligence': 10,\r\n",
    " 'from': 11,\r\n",
    " 'with': 12,\r\n",
    " 'artificial': 13,\r\n",
    " 'data': 14,\r\n",
    " 'this': 15,\r\n",
    " 'as': 16,\r\n",
    " 'machine': 17,\r\n",
    " 'for': 18,\r\n",
    " 'an': 19,\r\n",
    " 'you': 20,\r\n",
    " 'skills': 21,\r\n",
    " 'on': 22,\r\n",
    " '2023': 23,\r\n",
    " 'how': 24,\r\n",
    " 'learn': 25,\r\n",
    " 'what': 26,\r\n",
    " 'are': 27,\r\n",
    " 'can': 28,\r\n",
    " 'november': 29,\r\n",
    " 'average': 30,\r\n",
    " 'that': 31,\r\n",
    " 'science': 32,\r\n",
    " 'article': 33,\r\n",
    " 'we': 34,\r\n",
    " 'also': 35,\r\n",
    " 'these': 36,\r\n",
    " 'may': 37,\r\n",
    " 'experts': 38,\r\n",
    " 'human': 39,\r\n",
    " 'chatgpt': 40,\r\n",
    " 'according': 41,\r\n",
    " 'like': 42,\r\n",
    " 'more': 43,\r\n",
    " 'or': 44,\r\n",
    " 'well': 45,\r\n",
    " 'field': 46,\r\n",
    " 'its': 47,\r\n",
    " 'while': 48,\r\n",
    " 'deep': 49,\r\n",
    " 'demand': 50,\r\n",
    " 'compensation': 51,\r\n",
    " 'glassdoor': 52,\r\n",
    " 'salary': 53,\r\n",
    " '153': 54,\r\n",
    " '719': 55,\r\n",
    " 'per': 56,\r\n",
    " 'annum': 57,\r\n",
    " 'guide': 58,\r\n",
    " 'out': 59,\r\n",
    " 'need': 60,\r\n",
    " 'about': 61,\r\n",
    " 'at': 62,\r\n",
    " 'tools': 63,\r\n",
    " 'into': 64,\r\n",
    " 'than': 65,\r\n",
    " 'engineer': 66,\r\n",
    " 'tasks': 67,\r\n",
    " 'broad': 68,\r\n",
    " 'different': 69,\r\n",
    " 'ability': 70,\r\n",
    " 'future': 71,\r\n",
    " 'work': 72,\r\n",
    " 'terms': 73,\r\n",
    " '”': 74,\r\n",
    " 'over': 75,\r\n",
    " 'growing': 76,\r\n",
    " 'jobs': 77,\r\n",
    " 'seen': 78,\r\n",
    " 'increase': 79,\r\n",
    " 'scratch': 80,\r\n",
    " '2024': 81,\r\n",
    " 'find': 82,\r\n",
    " 'get': 83,\r\n",
    " 'insights': 84,\r\n",
    " 'industry': 85,\r\n",
    " 'where': 86,\r\n",
    " 'just': 87,\r\n",
    " 'business': 88,\r\n",
    " 'will': 89,\r\n",
    " 'their': 90,\r\n",
    " 'by': 91,\r\n",
    " 'if': 92,\r\n",
    " 'explore': 93,\r\n",
    " 'computer': 94,\r\n",
    " 'focused': 95,\r\n",
    " 'systems': 96,\r\n",
    " 'perform': 97,\r\n",
    " 'range': 98,\r\n",
    " 'language': 99,\r\n",
    " 'decisions': 100,\r\n",
    " 'each': 101,\r\n",
    " 'types': 102,\r\n",
    " 'technology': 103,\r\n",
    " 'be': 104,\r\n",
    " 'ani': 105,\r\n",
    " 'interact': 106,\r\n",
    " 'agi': 107,\r\n",
    " 'across': 108,\r\n",
    " 'level': 109,\r\n",
    " 'such': 110,\r\n",
    " 'many': 111,\r\n",
    " 'concept': 112,\r\n",
    " 'asi': 113,\r\n",
    " 'refers': 114,\r\n",
    " 'all': 115,\r\n",
    " 'there': 116,\r\n",
    " \"isn't\": 117,\r\n",
    " 'subset': 118,\r\n",
    " 'algorithms': 119,\r\n",
    " 'it': 120,\r\n",
    " 'news': 121,\r\n",
    " 'now': 122,\r\n",
    " \"it's\": 123,\r\n",
    " 'make': 124,\r\n",
    " 'better': 125,\r\n",
    " 'fast': 126,\r\n",
    " 'years': 127,\r\n",
    " 'specialists': 128,\r\n",
    " 'high': 129,\r\n",
    " 'paying': 130,\r\n",
    " 'job': 131,\r\n",
    " 'naturally': 132,\r\n",
    " 'surge': 133,\r\n",
    " 'comes': 134,\r\n",
    " 'attractive': 135,\r\n",
    " 'united': 136,\r\n",
    " 'states': 137,\r\n",
    " 'potential': 138,\r\n",
    " 'bonuses': 139,\r\n",
    " 'profit': 140,\r\n",
    " 'sharing': 141,\r\n",
    " 'engineers': 142,\r\n",
    " 'scientists': 143,\r\n",
    " 'similarly': 144,\r\n",
    " 'paid': 145,\r\n",
    " 'salaries': 146,\r\n",
    " '151': 147,\r\n",
    " '158': 148,\r\n",
    " '178': 149,\r\n",
    " '515': 150,\r\n",
    " 'respectively': 151,\r\n",
    " 'financial': 152,\r\n",
    " 'reflects': 153,\r\n",
    " 'value': 154,\r\n",
    " 'impact': 155,\r\n",
    " 'marketplace': 156,\r\n",
    " \"we've\": 157,\r\n",
    " 'averages': 158,\r\n",
    " 'when': 159,\r\n",
    " 'wrote': 160,\r\n",
    " 'last': 161,\r\n",
    " 'update': 162,\r\n",
    " 'was': 163,\r\n",
    " 'reported': 164,\r\n",
    " '128': 165,\r\n",
    " '479': 166,\r\n",
    " 'complete': 167,\r\n",
    " 'everything': 168,\r\n",
    " 'know': 169,\r\n",
    " 'tips': 170,\r\n",
    " 'started': 171,\r\n",
    " 'helpful': 172,\r\n",
    " 'resources': 173,\r\n",
    " 'we’re': 174,\r\n",
    " 'living': 175,\r\n",
    " 'through': 176,\r\n",
    " 'quite': 177,\r\n",
    " 'possibly': 178,\r\n",
    " 'pivotal': 179,\r\n",
    " 'point': 180,\r\n",
    " 'history': 181,\r\n",
    " 'importance': 182,\r\n",
    " 'becoming': 183,\r\n",
    " 'increasingly': 184,\r\n",
    " 'undeniable': 185,\r\n",
    " 'consider': 186,\r\n",
    " 'statistic': 187,\r\n",
    " '97': 188,\r\n",
    " 'owners': 189,\r\n",
    " 'expect': 190,\r\n",
    " 'bring': 191,\r\n",
    " 'positive': 192,\r\n",
    " 'changes': 193,\r\n",
    " 'least': 194,\r\n",
    " 'one': 195,\r\n",
    " 'area': 196,\r\n",
    " 'survey': 197,\r\n",
    " 'forbes': 198,\r\n",
    " 'advisor': 199,\r\n",
    " 'moreover': 200,\r\n",
    " 'midjourney': 201,\r\n",
    " 'bard': 202,\r\n",
    " 'ushering': 203,\r\n",
    " 'mainstream': 204,\r\n",
    " 'makes': 205,\r\n",
    " 'art': 206,\r\n",
    " 'relevant': 207,\r\n",
    " 'ever': 208,\r\n",
    " 'before': 209,\r\n",
    " \"you're\": 210,\r\n",
    " 'aspiring': 211,\r\n",
    " 'scientist': 212,\r\n",
    " 'researcher': 213,\r\n",
    " 'simply': 214,\r\n",
    " 'enthusiast': 215,\r\n",
    " 'throughout': 216,\r\n",
    " \"we'll\": 217,\r\n",
    " 'detail': 218,\r\n",
    " 'offer': 219,\r\n",
    " 'help': 220,\r\n",
    " 'steer': 221,\r\n",
    " 'your': 222,\r\n",
    " 'journey': 223,\r\n",
    " 'covering': 224,\r\n",
    " 'you’ll': 225,\r\n",
    " 'master': 226,\r\n",
    " 'businesses': 227,\r\n",
    " 'leverage': 228,\r\n",
    " 'today’s': 229,\r\n",
    " 'landscape': 230,\r\n",
    " 'branch': 231,\r\n",
    " 'creating': 232,\r\n",
    " 'would': 233,\r\n",
    " 'normally': 234,\r\n",
    " 'require': 235,\r\n",
    " 'understanding': 236,\r\n",
    " 'natural': 237,\r\n",
    " 'recognizing': 238,\r\n",
    " 'patterns': 239,\r\n",
    " 'making': 240,\r\n",
    " 'experience': 241,\r\n",
    " 'numerous': 242,\r\n",
    " 'subfields': 243,\r\n",
    " 'unique': 244,\r\n",
    " 'objectives': 245,\r\n",
    " 'specializations': 246,\r\n",
    " 'check': 247,\r\n",
    " 'our': 248,\r\n",
    " 'full': 249,\r\n",
    " 'separate': 250,\r\n",
    " 'grows': 251,\r\n",
    " 'popularity': 252,\r\n",
    " 'discussed': 253,\r\n",
    " 'various': 254,\r\n",
    " 'ways': 255,\r\n",
    " 'simplify': 256,\r\n",
    " 'remainder': 257,\r\n",
    " 'it’s': 258,\r\n",
    " 'important': 259,\r\n",
    " 'look': 260,\r\n",
    " 'categorized': 261,\r\n",
    " 'three': 262,\r\n",
    " 'levels': 263,\r\n",
    " 'based': 264,\r\n",
    " 'capabilities': 265,\r\n",
    " 'narrow': 266,\r\n",
    " 'most': 267,\r\n",
    " 'common': 268,\r\n",
    " 'form': 269,\r\n",
    " 'today': 270,\r\n",
    " 'designed': 271,\r\n",
    " 'single': 272,\r\n",
    " 'task': 273,\r\n",
    " 'voice': 274,\r\n",
    " 'recognition': 275,\r\n",
    " 'recommendations': 276,\r\n",
    " 'streaming': 277,\r\n",
    " 'services': 278,\r\n",
    " 'general': 279,\r\n",
    " 'possesses': 280,\r\n",
    " 'understand': 281,\r\n",
    " 'adapt': 282,\r\n",
    " 'implement': 283,\r\n",
    " 'knowledge': 284,\r\n",
    " 'wide': 285,\r\n",
    " 'large': 286,\r\n",
    " 'models': 287,\r\n",
    " 'have': 288,\r\n",
    " 'shown': 289,\r\n",
    " 'generalize': 290,\r\n",
    " 'tasks—as': 291,\r\n",
    " 'still': 292,\r\n",
    " 'theoretical': 293,\r\n",
    " 'super': 294,\r\n",
    " 'final': 295,\r\n",
    " 'scenario': 296,\r\n",
    " 'surpasses': 297,\r\n",
    " 'nearly': 298,\r\n",
    " 'economically': 299,\r\n",
    " 'valuable': 300,\r\n",
    " 'intriguing': 301,\r\n",
    " 'remains': 302,\r\n",
    " 'largely': 303,\r\n",
    " 'speculative': 304,\r\n",
    " 'difference': 305,\r\n",
    " 'between': 306,\r\n",
    " 'new': 307,\r\n",
    " 'topic': 308,\r\n",
    " 'see': 309,\r\n",
    " '“machine': 310,\r\n",
    " '“deep': 311,\r\n",
    " '“data': 312,\r\n",
    " 'others': 313,\r\n",
    " 'creep': 314,\r\n",
    " 'discourse': 315,\r\n",
    " 'several': 316,\r\n",
    " 'subsets': 317,\r\n",
    " 'including': 318,\r\n",
    " 'ml': 319,\r\n",
    " 'dl': 320,\r\n",
    " 'official': 321,\r\n",
    " 'definition': 322,\r\n",
    " 'any': 323,\r\n",
    " 'argue': 324,\r\n",
    " 'exact': 325,\r\n",
    " 'boundaries': 326,\r\n",
    " 'consensus': 327,\r\n",
    " 'scope': 328,\r\n",
    " 'term': 329,\r\n",
    " 'here’s': 330,\r\n",
    " 'breakdown': 331,\r\n",
    " 'defined': 332,\r\n",
    " 'behave': 333,\r\n",
    " 'intelligently': 334,\r\n",
    " 'reason': 335,\r\n",
    " 'humans': 336,\r\n",
    " 'developing': 337,\r\n",
    " 'without': 338,\r\n",
    " 'explicitly': 339,\r\n",
    " 'being': 340,\r\n",
    " 'programmed': 341,\r\n",
    " 'responsible': 342,\r\n",
    " 'awe': 343,\r\n",
    " 'inspiring': 344,\r\n",
    " 'stories': 345,\r\n",
    " 'e': 346,\r\n",
    " 'g': 347,\r\n",
    " 'self': 348,\r\n",
    " 'driving': 349,\r\n",
    " 'cars': 350,\r\n",
    " 'inspired': 351,\r\n",
    " \"brain's\": 352,\r\n",
    " 'structure': 353,\r\n",
    " 'exceptionally': 354,\r\n",
    " 'unstructured': 355,\r\n",
    " 'images': 356,\r\n",
    " 'videos': 357,\r\n",
    " 'text': 358,\r\n",
    " 'cross': 359,\r\n",
    " 'disciplinary': 360,\r\n",
    " 'uses': 361,\r\n",
    " 'above': 362,\r\n",
    " 'amongst': 363,\r\n",
    " 'other': 364,\r\n",
    " 'analysis': 365,\r\n",
    " 'statistics': 366,\r\n",
    " 'visualization': 367,\r\n",
    " 'insight': 368,\r\n",
    " 'hy': 369,\r\n",
    " 'should': 370,\r\n",
    " 'right': 371,\r\n",
    " 'buzzword': 372,\r\n",
    " 'revolutionary': 373,\r\n",
    " 'transforming': 374,\r\n",
    " 'live': 375,\r\n",
    " 'explosion': 376,\r\n",
    " 'sense': 377,\r\n",
    " 'skyrocketing': 378,\r\n",
    " \"there's\": 379,\r\n",
    " 'no': 380,\r\n",
    " 'time': 381,\r\n",
    " 'start': 382,\r\n",
    " \"here's\": 383,\r\n",
    " 'why': 384,\r\n",
    " 'present': 385,\r\n",
    " 'number': 386,\r\n",
    " 'has': 387,\r\n",
    " 'significant': 388,\r\n",
    " 'growth': 389,\r\n",
    " 'recent': 390,\r\n",
    " 'world': 391,\r\n",
    " 'economic': 392,\r\n",
    " 'forum’s': 393,\r\n",
    " 'report': 394,\r\n",
    " '\\u200b\\u200bai': 395,\r\n",
    " 'top': 396,\r\n",
    " 'list': 397,\r\n",
    " 'next': 398,\r\n",
    " 'five': 399,\r\n",
    " 'industries': 400,\r\n",
    " 'continue': 401,\r\n",
    " 'adopt': 402,\r\n",
    " 'technologies': 403,\r\n",
    " 'streamline': 404,\r\n",
    " 'operations': 405,\r\n",
    " 'likely': 406,\r\n",
    " 'only': 407}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf7dcf-d633-4614-8a0e-3d3a02bc0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for sentence in datascience.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        sequence = token_list[:i+1]\n",
    "        input_sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc569ae9-494d-4495-a626-90676bd1979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4255363-fe32-4798-b726-04cf63cc0b01",
   "metadata": {},
   "source": [
    "[[24, 5],\n",
    " [24, 5, 25],\n",
    " [24, 5, 25, 2],\n",
    " [24, 5, 25, 2, 11],\n",
    " [24, 5, 25, 2, 11, 80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4799e-c16a-4c09-954e-51da68b69b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(input_sequence) for input_sequence in input_sequences])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36a2af-9d81-4a44-a712-f98114bcb114",
   "metadata": {},
   "source": [
    "81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2f2e3-d7d5-4ae8-b4a6-5435d33a80a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding\n",
    "input_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_length, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947d948-7ada-4c65-a65b-65176fef076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6c47f-8c78-4cb0-8852-a6eb95d448c8",
   "metadata": {},
   "source": [
    "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  5, 25], dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb807a-ef46-4f52-95c8-4028031041ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide dataset into feature set (X) and labels (y)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e8639-db64-467b-a44c-dbaa884f5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1858ebe-9eb2-4750-83a1-c09042184f52",
   "metadata": {},
   "source": [
    "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n",
    "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24], dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0706c-3f6d-4c6e-a3ed-e73b62f3a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f2585-1a85-4ce5-b818-5e749ff23724",
   "metadata": {},
   "source": [
    "array([ 5, 25,  2, ..., 55,  7, 29], dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f91e11-cb2d-40b6-aa90-f3bd1e39e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words\n",
    "num_classes = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7425846-28ae-4913-ad91-82ca698e2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding labels\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a54ea-faf6-479f-a925-41f53bd69c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9d8a5-c990-4249-af4f-46ee62cb63ba",
   "metadata": {},
   "source": [
    "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n",
    "      dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f39525-0cee-49e8-8e48-33c5eed1d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(num_classes, 80, input_length=max_length-1))\n",
    "model.add(tf.keras.layers.LSTM(100))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a669c-38e6-41ab-b6d4-ed212ea2102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33413850-586e-4f7f-b607-7668879dbc6c",
   "metadata": {},
   "source": [
    "Model: \"sequential\"\r\n",
    "_________________________________________________________________\r\n",
    " Layer (type)                Output Shape              Param #   \r\n",
    "=================================================================\r\n",
    " embedding (Embedding)       (None, 80, 80)            32640     \r\n",
    "                                                                 \r\n",
    " lstm (LSTM)                 (None, 100)               72400     \r\n",
    "                                                                 \r\n",
    " dense (Dense)               (None, 408)               41208     \r\n",
    "                                                                 \r\n",
    "=================================================================\r\n",
    "Total params: 146248 (571.28 KB)\r\n",
    "Trainable params: 146248 (571.28 KB)\r\n",
    "Non-trainable params: 0 (0.00 Byte)\r\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b224f-667d-42a8-8abc-9eabcd5b586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84de5b2-adca-4cda-95d8-d1840297954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "history = model.fit(X, y, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c032154-331c-47fc-91f2-5612d9570160",
   "metadata": {},
   "source": [
    "Epoch 1/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0945 - accuracy: 0.9892\r\n",
    "Epoch 2/150\r\n",
    "32/32 [==============================] - 2s 76ms/step - loss: 0.0915 - accuracy: 0.9902\r\n",
    "Epoch 3/150\r\n",
    "32/32 [==============================] - 3s 100ms/step - loss: 0.0901 - accuracy: 0.9892\r\n",
    "Epoch 4/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0876 - accuracy: 0.9882\r\n",
    "Epoch 5/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0863 - accuracy: 0.9902\r\n",
    "Epoch 6/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0846 - accuracy: 0.9902\r\n",
    "Epoch 7/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0823 - accuracy: 0.9892\r\n",
    "Epoch 8/150\r\n",
    "32/32 [==============================] - 3s 84ms/step - loss: 0.0806 - accuracy: 0.9902\r\n",
    "Epoch 9/150\r\n",
    "32/32 [==============================] - 3s 96ms/step - loss: 0.0791 - accuracy: 0.9902\r\n",
    "Epoch 10/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0778 - accuracy: 0.9882\r\n",
    "Epoch 11/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0763 - accuracy: 0.9882\r\n",
    "Epoch 12/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0751 - accuracy: 0.9882\r\n",
    "Epoch 13/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0732 - accuracy: 0.9892\r\n",
    "Epoch 14/150\r\n",
    "32/32 [==============================] - 3s 98ms/step - loss: 0.0715 - accuracy: 0.9902\r\n",
    "Epoch 15/150\r\n",
    "32/32 [==============================] - 4s 118ms/step - loss: 0.0703 - accuracy: 0.9892\r\n",
    "Epoch 16/150\r\n",
    "32/32 [==============================] - 3s 82ms/step - loss: 0.0693 - accuracy: 0.9892\r\n",
    "Epoch 17/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0675 - accuracy: 0.9892\r\n",
    "Epoch 18/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0665 - accuracy: 0.9902\r\n",
    "Epoch 19/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0664 - accuracy: 0.9892\r\n",
    "Epoch 20/150\r\n",
    "32/32 [==============================] - 3s 92ms/step - loss: 0.0647 - accuracy: 0.9892\r\n",
    "Epoch 21/150\r\n",
    "32/32 [==============================] - 3s 85ms/step - loss: 0.0634 - accuracy: 0.9882\r\n",
    "Epoch 22/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0623 - accuracy: 0.9882\r\n",
    "Epoch 23/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0611 - accuracy: 0.9892\r\n",
    "Epoch 24/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0598 - accuracy: 0.9902\r\n",
    "Epoch 25/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0593 - accuracy: 0.9902\r\n",
    "Epoch 26/150\r\n",
    "32/32 [==============================] - 3s 101ms/step - loss: 0.0584 - accuracy: 0.9902\r\n",
    "Epoch 27/150\r\n",
    "32/32 [==============================] - 2s 72ms/step - loss: 0.0572 - accuracy: 0.9892\r\n",
    "Epoch 28/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0565 - accuracy: 0.9892\r\n",
    "Epoch 29/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0558 - accuracy: 0.9882\r\n",
    "Epoch 30/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0552 - accuracy: 0.9882\r\n",
    "Epoch 31/150\r\n",
    "32/32 [==============================] - 2s 75ms/step - loss: 0.0542 - accuracy: 0.9902\r\n",
    "Epoch 32/150\r\n",
    "32/32 [==============================] - 3s 104ms/step - loss: 0.0535 - accuracy: 0.9892\r\n",
    "Epoch 33/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0521 - accuracy: 0.9882\r\n",
    "Epoch 34/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0512 - accuracy: 0.9892\r\n",
    "Epoch 35/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0507 - accuracy: 0.9902\r\n",
    "Epoch 36/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0509 - accuracy: 0.9892\r\n",
    "Epoch 37/150\r\n",
    "32/32 [==============================] - 3s 82ms/step - loss: 0.0498 - accuracy: 0.9902\r\n",
    "Epoch 38/150\r\n",
    "32/32 [==============================] - 3s 98ms/step - loss: 0.0489 - accuracy: 0.9902\r\n",
    "Epoch 39/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0492 - accuracy: 0.9892\r\n",
    "Epoch 40/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0479 - accuracy: 0.9892\r\n",
    "Epoch 41/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0470 - accuracy: 0.9892\r\n",
    "Epoch 42/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0469 - accuracy: 0.9892\r\n",
    "Epoch 43/150\r\n",
    "32/32 [==============================] - 3s 90ms/step - loss: 0.0454 - accuracy: 0.9892\r\n",
    "Epoch 44/150\r\n",
    "32/32 [==============================] - 3s 88ms/step - loss: 0.0461 - accuracy: 0.9892\r\n",
    "Epoch 45/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0452 - accuracy: 0.9882\r\n",
    "Epoch 46/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0451 - accuracy: 0.9882\r\n",
    "Epoch 47/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0441 - accuracy: 0.9882\r\n",
    "Epoch 48/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0445 - accuracy: 0.9892\r\n",
    "Epoch 49/150\r\n",
    "32/32 [==============================] - 3s 100ms/step - loss: 0.0435 - accuracy: 0.9882\r\n",
    "Epoch 50/150\r\n",
    "32/32 [==============================] - 3s 78ms/step - loss: 0.0430 - accuracy: 0.9892\r\n",
    "Epoch 51/150\r\n",
    "32/32 [==============================] - 2s 71ms/step - loss: 0.0424 - accuracy: 0.9902\r\n",
    "Epoch 52/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0419 - accuracy: 0.9902\r\n",
    "Epoch 53/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0420 - accuracy: 0.9882\r\n",
    "Epoch 54/150\r\n",
    "32/32 [==============================] - 2s 76ms/step - loss: 0.0412 - accuracy: 0.9892\r\n",
    "Epoch 55/150\r\n",
    "32/32 [==============================] - 3s 104ms/step - loss: 0.0409 - accuracy: 0.9892\r\n",
    "Epoch 56/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0411 - accuracy: 0.9892\r\n",
    "Epoch 57/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0399 - accuracy: 0.9882\r\n",
    "Epoch 58/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0400 - accuracy: 0.9902\r\n",
    "Epoch 59/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0400 - accuracy: 0.9882\r\n",
    "Epoch 60/150\r\n",
    "32/32 [==============================] - 3s 85ms/step - loss: 0.0391 - accuracy: 0.9892\r\n",
    "Epoch 61/150\r\n",
    "32/32 [==============================] - 3s 96ms/step - loss: 0.0381 - accuracy: 0.9892\r\n",
    "Epoch 62/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0384 - accuracy: 0.9902\r\n",
    "Epoch 63/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0376 - accuracy: 0.9892\r\n",
    "Epoch 64/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0376 - accuracy: 0.9882\r\n",
    "Epoch 65/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0376 - accuracy: 0.9882\r\n",
    "Epoch 66/150\r\n",
    "32/32 [==============================] - 3s 97ms/step - loss: 0.0372 - accuracy: 0.9902\r\n",
    "Epoch 67/150\r\n",
    "32/32 [==============================] - 3s 85ms/step - loss: 0.0373 - accuracy: 0.9902\r\n",
    "Epoch 68/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0364 - accuracy: 0.9892\r\n",
    "Epoch 69/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0358 - accuracy: 0.9892\r\n",
    "Epoch 70/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0355 - accuracy: 0.9902\r\n",
    "Epoch 71/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0353 - accuracy: 0.9892\r\n",
    "Epoch 72/150\r\n",
    "32/32 [==============================] - 3s 105ms/step - loss: 0.0351 - accuracy: 0.9892\r\n",
    "Epoch 73/150\r\n",
    "32/32 [==============================] - 2s 74ms/step - loss: 0.0353 - accuracy: 0.9892\r\n",
    "Epoch 74/150\r\n",
    "32/32 [==============================] - 2s 67ms/step - loss: 0.0344 - accuracy: 0.9882\r\n",
    "Epoch 75/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0344 - accuracy: 0.9892\r\n",
    "Epoch 76/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0344 - accuracy: 0.9892\r\n",
    "Epoch 77/150\r\n",
    "32/32 [==============================] - 2s 74ms/step - loss: 0.0339 - accuracy: 0.9892\r\n",
    "Epoch 78/150\r\n",
    "32/32 [==============================] - 3s 106ms/step - loss: 0.0340 - accuracy: 0.9882\r\n",
    "Epoch 79/150\r\n",
    "32/32 [==============================] - 2s 72ms/step - loss: 0.0335 - accuracy: 0.9882\r\n",
    "Epoch 80/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0335 - accuracy: 0.9882\r\n",
    "Epoch 81/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0340 - accuracy: 0.9892\r\n",
    "Epoch 82/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0333 - accuracy: 0.9892\r\n",
    "Epoch 83/150\r\n",
    "32/32 [==============================] - 3s 87ms/step - loss: 0.0325 - accuracy: 0.9882\r\n",
    "Epoch 84/150\r\n",
    "32/32 [==============================] - 3s 91ms/step - loss: 0.0326 - accuracy: 0.9882\r\n",
    "Epoch 85/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0323 - accuracy: 0.9892\r\n",
    "Epoch 86/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0322 - accuracy: 0.9882\r\n",
    "Epoch 87/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0324 - accuracy: 0.9882\r\n",
    "Epoch 88/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0318 - accuracy: 0.9892\r\n",
    "Epoch 89/150\r\n",
    "32/32 [==============================] - 3s 100ms/step - loss: 0.0310 - accuracy: 0.9902\r\n",
    "Epoch 90/150\r\n",
    "32/32 [==============================] - 3s 79ms/step - loss: 0.0310 - accuracy: 0.9902\r\n",
    "Epoch 91/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0318 - accuracy: 0.9892\r\n",
    "Epoch 92/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0307 - accuracy: 0.9902\r\n",
    "Epoch 93/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0304 - accuracy: 0.9902\r\n",
    "Epoch 94/150\r\n",
    "32/32 [==============================] - 2s 78ms/step - loss: 0.0305 - accuracy: 0.9892\r\n",
    "Epoch 95/150\r\n",
    "32/32 [==============================] - 3s 106ms/step - loss: 0.0309 - accuracy: 0.9902\r\n",
    "Epoch 96/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0307 - accuracy: 0.9882\r\n",
    "Epoch 97/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0299 - accuracy: 0.9892\r\n",
    "Epoch 98/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0303 - accuracy: 0.9882\r\n",
    "Epoch 99/150\r\n",
    "32/32 [==============================] - 2s 72ms/step - loss: 0.0294 - accuracy: 0.9892\r\n",
    "Epoch 100/150\r\n",
    "32/32 [==============================] - 3s 91ms/step - loss: 0.0295 - accuracy: 0.9902\r\n",
    "Epoch 101/150\r\n",
    "32/32 [==============================] - 3s 92ms/step - loss: 0.0292 - accuracy: 0.9902\r\n",
    "Epoch 102/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0290 - accuracy: 0.9892\r\n",
    "Epoch 103/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0295 - accuracy: 0.9892\r\n",
    "Epoch 104/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0295 - accuracy: 0.9892\r\n",
    "Epoch 105/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0291 - accuracy: 0.9902\r\n",
    "Epoch 106/150\r\n",
    "32/32 [==============================] - 3s 103ms/step - loss: 0.0288 - accuracy: 0.9882\r\n",
    "Epoch 107/150\r\n",
    "32/32 [==============================] - 2s 75ms/step - loss: 0.0284 - accuracy: 0.9892\r\n",
    "Epoch 108/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0289 - accuracy: 0.9882\r\n",
    "Epoch 109/150\r\n",
    "32/32 [==============================] - 2s 71ms/step - loss: 0.0290 - accuracy: 0.9882\r\n",
    "Epoch 110/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0281 - accuracy: 0.9902\r\n",
    "Epoch 111/150\r\n",
    "32/32 [==============================] - 3s 88ms/step - loss: 0.0279 - accuracy: 0.9902\r\n",
    "Epoch 112/150\r\n",
    "32/32 [==============================] - 3s 102ms/step - loss: 0.0280 - accuracy: 0.9882\r\n",
    "Epoch 113/150\r\n",
    "32/32 [==============================] - 2s 72ms/step - loss: 0.0282 - accuracy: 0.9892\r\n",
    "Epoch 114/150\r\n",
    "32/32 [==============================] - 2s 72ms/step - loss: 0.0282 - accuracy: 0.9902\r\n",
    "Epoch 115/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0282 - accuracy: 0.9882\r\n",
    "Epoch 116/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0276 - accuracy: 0.9902\r\n",
    "Epoch 117/150\r\n",
    "32/32 [==============================] - 3s 103ms/step - loss: 0.0283 - accuracy: 0.9892\r\n",
    "Epoch 118/150\r\n",
    "32/32 [==============================] - 3s 81ms/step - loss: 0.0279 - accuracy: 0.9882\r\n",
    "Epoch 119/150\r\n",
    "32/32 [==============================] - 2s 68ms/step - loss: 0.0284 - accuracy: 0.9872\r\n",
    "Epoch 120/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0271 - accuracy: 0.9892\r\n",
    "Epoch 121/150\r\n",
    "32/32 [==============================] - 3s 101ms/step - loss: 0.0271 - accuracy: 0.9902\r\n",
    "Epoch 122/150\r\n",
    "32/32 [==============================] - 3s 106ms/step - loss: 0.0268 - accuracy: 0.9892\r\n",
    "Epoch 123/150\r\n",
    "32/32 [==============================] - 3s 85ms/step - loss: 0.0267 - accuracy: 0.9902\r\n",
    "Epoch 124/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0269 - accuracy: 0.9892\r\n",
    "Epoch 125/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0269 - accuracy: 0.9902\r\n",
    "Epoch 126/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0269 - accuracy: 0.9892\r\n",
    "Epoch 127/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0271 - accuracy: 0.9892\r\n",
    "Epoch 128/150\r\n",
    "32/32 [==============================] - 3s 105ms/step - loss: 0.0272 - accuracy: 0.9892\r\n",
    "Epoch 129/150\r\n",
    "32/32 [==============================] - 2s 76ms/step - loss: 0.0262 - accuracy: 0.9892\r\n",
    "Epoch 130/150\r\n",
    "32/32 [==============================] - 2s 72ms/step - loss: 0.0262 - accuracy: 0.9892\r\n",
    "Epoch 131/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0262 - accuracy: 0.9892\r\n",
    "Epoch 132/150\r\n",
    "32/32 [==============================] - 2s 71ms/step - loss: 0.0262 - accuracy: 0.9882\r\n",
    "Epoch 133/150\r\n",
    "32/32 [==============================] - 3s 84ms/step - loss: 0.0261 - accuracy: 0.9892\r\n",
    "Epoch 134/150\r\n",
    "32/32 [==============================] - 3s 100ms/step - loss: 0.0265 - accuracy: 0.9892\r\n",
    "Epoch 135/150\r\n",
    "32/32 [==============================] - 2s 75ms/step - loss: 0.0269 - accuracy: 0.9882\r\n",
    "Epoch 136/150\r\n",
    "32/32 [==============================] - 2s 71ms/step - loss: 0.0258 - accuracy: 0.9892\r\n",
    "Epoch 137/150\r\n",
    "32/32 [==============================] - 2s 73ms/step - loss: 0.0263 - accuracy: 0.9902\r\n",
    "Epoch 138/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0255 - accuracy: 0.9902\r\n",
    "Epoch 139/150\r\n",
    "32/32 [==============================] - 3s 103ms/step - loss: 0.0255 - accuracy: 0.9892\r\n",
    "Epoch 140/150\r\n",
    "32/32 [==============================] - 3s 79ms/step - loss: 0.0262 - accuracy: 0.9902\r\n",
    "Epoch 141/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0249 - accuracy: 0.9892\r\n",
    "Epoch 142/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0254 - accuracy: 0.9902\r\n",
    "Epoch 143/150\r\n",
    "32/32 [==============================] - 2s 69ms/step - loss: 0.0259 - accuracy: 0.9892\r\n",
    "Epoch 144/150\r\n",
    "32/32 [==============================] - 3s 81ms/step - loss: 0.0257 - accuracy: 0.9892\r\n",
    "Epoch 145/150\r\n",
    "32/32 [==============================] - 3s 104ms/step - loss: 0.0253 - accuracy: 0.9882\r\n",
    "Epoch 146/150\r\n",
    "32/32 [==============================] - 2s 70ms/step - loss: 0.0254 - accuracy: 0.9892\r\n",
    "Epoch 147/150\r\n",
    "32/32 [==============================] - 2s 72ms/step - loss: 0.0250 - accuracy: 0.9892\r\n",
    "Epoch 148/150\r\n",
    "32/32 [==============================] - 2s 73ms/step - loss: 0.0253 - accuracy: 0.9892\r\n",
    "Epoch 149/150\r\n",
    "32/32 [==============================] - 2s 73ms/step - loss: 0.0251 - accuracy: 0.9892\r\n",
    "Epoch 150/150\r\n",
    "32/32 [==============================] - 3s 99ms/step - loss: 0.0247 - accuracy: 0.9902)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd79ca-57d0-4243-805d-cce87f04fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What is Artificial Intelligence (AI)?\"\n",
    "for i in range(11):\n",
    "    token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_length-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    input_text += \" \" + output_word\n",
    "\n",
    "print(input_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e007f5d-ef6d-42c2-a8c5-bf550b75ebf0",
   "metadata": {},
   "source": [
    "1/1 [==============================] - 0s 32ms/step\r\n",
    "1/1 [==============================] - 0s 32ms/step\r\n",
    "1/1 [==============================] - 0s 31ms/step\r\n",
    "1/1 [==============================] - 0s 32ms/step\r\n",
    "1/1 [==============================] - 0s 33ms/step\r\n",
    "1/1 [==============================] - 0s 32ms/step\r\n",
    "1/1 [==============================] - 0s 32ms/step\r\n",
    "1/1 [==============================] - 0s 38ms/step\r\n",
    "1/1 [==============================] - 0s 31ms/step\r\n",
    "1/1 [==============================] - 0s 31ms/step\r\n",
    "1/1 [==============================] - 0s 32ms/step\r\n",
    "What is Artificial Intelligence (AI)? is a branch of computer science focused on creating sms that\r\n",
    "[ ]\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
